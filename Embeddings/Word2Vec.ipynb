{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6W4B4sZSVSt1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, random, time\n",
    "import gensim, spacy, nltk, re, string\n",
    "from collections import Counter\n",
    "from sklearn.utils import shuffle\n",
    "import unicodedata as ud\n",
    "from IPython import display\n",
    "import pickle, json\n",
    "import progressbar\n",
    "%matplotlib inline\n",
    "from gensim.models import Word2Vec, FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eaqt3NViQcqr"
   },
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "G_39ZJyhOAta"
   },
   "outputs": [],
   "source": [
    "class word2vec():\n",
    "\n",
    "  def __init__(self, corpus=None, window=5, size=50, k=2):\n",
    "    self.Center = None\n",
    "    self.Context = None\n",
    "    self.h = int(np.floor(window/2))\n",
    "    self.k = k\n",
    "    self.size = size\n",
    "    self.vocab = None\n",
    "    self.weights = None\n",
    "    if corpus is not None:\n",
    "      self.preprocess(corpus)\n",
    "\n",
    "  def fit(self, corpus=None, epochs=5, train=True, lr=0.001):\n",
    "    if self.vocab is None:\n",
    "      self.preprocess(corpus)\n",
    "\n",
    "    if train:\n",
    "      self.train(corpus, epochs, lr)\n",
    "\n",
    "  def preprocess(self, corpus):\n",
    "    tokens = [token for row in corpus for token in row]\n",
    "    num_tokens = len(tokens)\n",
    "    counter = Counter(tokens)\n",
    "    self.vocab = list(counter.keys())\n",
    "    self.weights = [(count/num_tokens)**0.75 for count in counter.values()]\n",
    "    self.weights /= np.sum(self.weights)\n",
    "    self.initialize_embeddings()\n",
    "\n",
    "  def dot(self, C, X):\n",
    "    return np.dot(C, X)\n",
    "\n",
    "  def loss(self, p, y):\n",
    "    return -y*np.log(p) -(1-y)*np.log(1-p)\n",
    "\n",
    "  def sigmoid(self, z):\n",
    "    return 1/(1+np.exp(z))\n",
    "\n",
    "  def feedforward(self, C, X):\n",
    "    return self.sigmoid(self.dot(C, X))\n",
    "\n",
    "  def backprop(self, c, x, y, p, lr=0.001):\n",
    "    delta_C = lr*(y-p)*self.Center[c]\n",
    "    delta_X = lr*(y-p)*self.Context[x]\n",
    "\n",
    "    self.update_embeddings(c, x, delta_C, delta_X)\n",
    "\n",
    "  def update_embeddings(self, c, x, delta_C, delta_X):\n",
    "    self.Center[c] -= delta_C\n",
    "    self.Context[x] -= delta_X\n",
    "\n",
    "  def initialize_embeddings(self):\n",
    "    if self.Center is None:\n",
    "      self.Center = dict([(token, np.random.rand(self.size, )*0.1) for token in self.vocab])\n",
    "    if self.Context is None:\n",
    "      self.Context = dict([(token, np.random.rand(self.size, )*0.1) for token in self.vocab])\n",
    "\n",
    "  def get_context(self, ind, token, row):\n",
    "    h = np.random.randint(1, self.h)\n",
    "    return  [row[i] for i in range(max(0,ind-h),max(0,ind))] + [row[i] for i in range(min(ind+1, len(row)),min(ind+h+1,len(row)))]\n",
    "\n",
    "  def negative_sample(self, token, context):\n",
    "    return [self.sample(token, context) for i in range(self.h*self.k)]\n",
    "\n",
    "  def sample(self, token, context):\n",
    "    sampled_token = np.random.choice(self.vocab, p=self.weights)\n",
    "    if sampled_token not in context+[token]:\n",
    "      return sampled_token\n",
    "    else:\n",
    "      return self.sample(token, context)\n",
    "\n",
    "  def train(self, corpus, epochs=5, lr=0.001):\n",
    "    bar = progressbar.ProgressBar(maxval=10, widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "      print(f'Epoch: {epoch}')\n",
    "      iter = list(np.linspace(0, len(corpus), 10).astype(int))\n",
    "      corpus = shuffle(corpus)\n",
    "      loss = []\n",
    "      start_time = time.time()\n",
    "\n",
    "      for j, row in enumerate(corpus):\n",
    "\n",
    "        for i,c in enumerate(row):\n",
    "          context = self.get_context(i, c, row)\n",
    "          negative_sample = self.negative_sample(c, context)\n",
    "          for (c, x), y in [((c, x), 1) for x in context] + [((c, x), 0) for x in negative_sample]:\n",
    "            p = self.feedforward(self.Center[c], self.Context[x])\n",
    "            self.backprop(c, x, y, p)\n",
    "            loss += [self.loss(p, y)]\n",
    "            \n",
    "        if j in iter: \n",
    "          bar.update(iter.index(j))\n",
    "\n",
    "      bar.finish()\n",
    "      print(f'\\nloss: {np.mean(loss)}, \\nepoch time: {self.time(start_time)}')\n",
    "      losses.append(np.mean(loss))\n",
    "\n",
    "  def time(self, start):\n",
    "    runtime = time.time()-start\n",
    "    hours = runtime//3600\n",
    "    runtime = runtime - 3600*hours\n",
    "    minutes = runtime//60\n",
    "    seconds = runtime - 60*minutes\n",
    "    return ':'.join([str(int(t)) for t in [hours, minutes, seconds]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQz-TKV6ENCt"
   },
   "source": [
    "### Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "oVii5wkcTOA0"
   },
   "outputs": [],
   "source": [
    "SIZE = 50\n",
    "EPOCHS = 20\n",
    "WINDOW = 5\n",
    "LR = 0.001\n",
    "K = 2\n",
    "CORPUS_PATH = '/content/drive/My Drive/NLP Urdu/data/corpus.txt'\n",
    "MODEL_PATH = '/content/drive/My Drive/NLP Urdu/Models/Word Embeddings/word2vec.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JtZAhYU__c4D"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "kvd6KWaJ-orU"
   },
   "outputs": [],
   "source": [
    "corpus = open(CORPUS_PATH).read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OURAsaIPAO_Q"
   },
   "outputs": [],
   "source": [
    "model = word2vec(window=WINDOW, size=SIZE, k=K)\n",
    "model.fit(corpus, epochs=EPOCHS, lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "unPsykBA_1Bz"
   },
   "outputs": [],
   "source": [
    "with open(MODEL_PATH, 'wb') as f:\n",
    "  dump(model, f)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Word2Vec.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
